{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead696a5",
   "metadata": {},
   "source": [
    "### 值迭代算法伪代码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7641807",
   "metadata": {},
   "source": [
    "![](assets/86.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a599649",
   "metadata": {},
   "source": [
    "### 例子\n",
    "(a)为待求解策略的网格世界，（b）为迭代一次得到的策略，（c）为迭代两次得到的策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1313933",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/87.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1cd68",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/88.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/89.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de103b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 值迭代算法中文流程：\n",
    "![](assets/90.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20295c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 上述例子代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_q_table(gamma: float, v_pi: np.ndarray) -> np.ndarray:\n",
    "    # 这里的-1, 0, 1对应着r_boundary、r_forbbiden，r_others和r_target\n",
    "    q_table = [\n",
    "        [-1 + gamma * v_pi[0], -1 + gamma * v_pi[1],  0 + gamma * v_pi[2], -1 + gamma * v_pi[0],  0 + gamma * v_pi[0]],\n",
    "        [-1 + gamma * v_pi[1], -1 + gamma * v_pi[1],  1 + gamma * v_pi[3],  0 + gamma * v_pi[0], -1 + gamma * v_pi[1]],\n",
    "        [ 0 + gamma * v_pi[0],  1 + gamma * v_pi[3], -1 + gamma * v_pi[2], -1 + gamma * v_pi[2],  0 + gamma * v_pi[2]],\n",
    "        [-1 + gamma * v_pi[1], -1 + gamma * v_pi[3], -1 + gamma * v_pi[3],  0 + gamma * v_pi[2],  1 + gamma * v_pi[3]],\n",
    "    ]\n",
    "    \n",
    "    return np.asarray(q_table)\n",
    "\n",
    "def solver():\n",
    "    r_boundary = -1         # 越界的reward为-1\n",
    "    r_forbbiden = -1        # 进入禁区的reward为-1\n",
    "    r_others = 0\n",
    "    r_target = 1            # 进入目标的reward为1\n",
    "    gamma = 0.9             # discounted rate为0.9（相对更加远视）\n",
    "    \n",
    "    v_pi = np.zeros(4)      # 初始的v_pi值全为0\n",
    "    action = np.empty(4)    # 初始化action（这里相当于策略pi)\n",
    "    \n",
    "    threshold = 1           # ||v_k+1 - v_k||小于阈值会退出循环\n",
    "    n_iters = 1000          # 循环次数，大于循环次数，即使||v_k+1 - v_k||没小于阈值也会退出循环\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        q_table = compute_q_table(gamma, v_pi)      # 计算q_table表，也就是对于每个state，采取action以后得到的action value\n",
    "        action = np.argmax(q_table, axis=1)         # 策略更新：获得action value最大位置的action作为新策略\n",
    "        v_pi_next = q_table[range(4), action]       \n",
    "        if (np.linalg.norm(v_pi - v_pi_next, ord=2)) < threshold:\n",
    "            break\n",
    "        v_pi = v_pi_next                            # 值更新：新的v_pi复制给旧的v_pi\n",
    "        \n",
    "    return action\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    action = solver()\n",
    "    print(f'The policy is:')\n",
    "    for i in range(4):\n",
    "        print(f'At state s{i + 1}, take action a{action[i] + 1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
