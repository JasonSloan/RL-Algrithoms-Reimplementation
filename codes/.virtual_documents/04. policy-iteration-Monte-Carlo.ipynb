





























# MC-Basic算法完整代码实现流程：
# # initialize a policy(actions of all the states)
# for i in n_outer_iterations:
#   for s in every states:
#       # fix the actions of all the states except current
#       for a in every actions:
#           # initialize an empty returns-list
#           for j in n_inner_iterations:
#               # generate an episode according to the probability and compute the discounted return based on current action and all other fixed actions
#               # append the result to the returns-list
#           end for
#           # get the action value for current action (expectation of all the discounted returns),
#           # namely, compute the mean value of the returns-list
#       end for
#   end for
#   # update the policy according to the action value
# end for

import numpy as np

def solver():
    # 该代码只用MC-Basic算法更新当前例子中的s1状态处的策略
    r_boundary = -1
    r_target = 1
    r_others = 0
    gamma = 0.9
    actions_map = {0: 'up', 1: 'right', 2: 'down', 3: 'left', 4: 'unchanged'}

    # 该例子中当前策略下s1-s9处的reward
    r_pi_k_table = [-1, 0, -1, 0, 0, 1, 0, 1, 1]
    q_pi0_s1a1 = -1 + gamma * (-1 / (1 - gamma))
    q_pi0_s1a2 = 0 + gamma ** 3 * (1 / (1 - gamma))
    q_pi0_s1a3 = 0 + gamma ** 3 * (1 / (1 - gamma))
    q_pi0_s1a4 = -1 + gamma * (-1 / (1 - gamma))
    q_pi0_s1a5 = 0 + gamma * (-1 / (1 - gamma))
    
    pi_0_s1 = np.array([q_pi0_s1a1, q_pi0_s1a2, q_pi0_s1a3, q_pi0_s1a4, q_pi0_s1a5]).argmax()
    print(f"Best action for state s1 is: {actions_map[pi_0_s1]}")
    
        

if __name__ == '__main__':
    solver()
