{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915b8e6",
   "metadata": {},
   "source": [
    "### A2C："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df412",
   "metadata": {},
   "source": [
    "![](assets/305.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011706a4-4eb2-466d-8640-dd0fde37493d",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6cbc8-afc4-4096-b7b1-f893fb93ad13",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/256.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bf0cc-267a-407f-b59a-118ae74e68c3",
   "metadata": {},
   "source": [
    "### 上述例子代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348c7ff-9bd7-4e24-9b5b-ca683707437d",
   "metadata": {},
   "source": [
    "例子：\n",
    "使用gym仿真库，gym官网: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb6df66-515e-4a09-92bd-2e4834cbd769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gym==0.15.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.16.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (4.8.1.78)\n",
      "Requirement already satisfied: future in c:\\users\\root\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.18.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: numpy in c:\\users\\root\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\users\\root\\appdata\\roaming\\python\\python311\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\root\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\root\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tqdm in c:\\users\\root\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\root\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.15.4\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "one_step_experience = namedtuple('one_step_experience', field_names=['current_observation', 'current_action', 'reward', 'next_observation'])\n",
    "\n",
    "\n",
    "class CustomGridWorld(gym.Env):\n",
    "    def __init__(self, grid_size=(5, 5), goal_position=(3, 2), forbidden_grids=None, action_space=5,\n",
    "                 forbidden_grids_penalty=-2, tgt_grid_reward=10, step_penalty=-1):\n",
    "        super(CustomGridWorld, self).__init__()\n",
    "        self.grid_size = grid_size  # (rows, cols)\n",
    "        self.goal_position = goal_position\n",
    "        self.forbidden_grids_penalty = forbidden_grids_penalty\n",
    "        self.tgt_grid_reward = tgt_grid_reward\n",
    "        self.step_penalty = step_penalty\n",
    "        self.action_space = spaces.Discrete(action_space)\n",
    "        self.observation_space = spaces.Discrete(grid_size[0] * grid_size[1])\n",
    "        self.state = (0, 0)\n",
    "        self.done = False\n",
    "        if forbidden_grids is None:\n",
    "            forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]\n",
    "        self.forbidden_grids = set(forbidden_grids)\n",
    "\n",
    "    def _get_state(self, observation):\n",
    "        cols = self.grid_size[1]\n",
    "        return (observation // cols, observation % cols)\n",
    "\n",
    "    def reset(self, init_observation=0):\n",
    "        self.state = self._get_state(init_observation)\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # Up\n",
    "            new_x, new_y = max(0, x - 1), y\n",
    "        elif action == 1:  # Right\n",
    "            new_x, new_y = x, min(self.grid_size[1] - 1, y + 1)\n",
    "        elif action == 2:  # Down\n",
    "            new_x, new_y = min(self.grid_size[0] - 1, x + 1), y\n",
    "        elif action == 3:  # Left\n",
    "            new_x, new_y = x, max(0, y - 1)\n",
    "        elif action == 4:  # Stay\n",
    "            new_x, new_y = x, y\n",
    "        else:\n",
    "            new_x, new_y = x, y\n",
    "\n",
    "        if new_x < 0 or new_x >= self.grid_size[0] or new_y < 0 or new_y >= self.grid_size[1]:\n",
    "            reward = self.step_penalty * 2  # Extra penalty for hitting wall, but not as severe as forbidden area\n",
    "            self.state = (x, y)\n",
    "        else:\n",
    "            self.state = (new_x, new_y)\n",
    "            if self.state == self.goal_position:\n",
    "                reward = self.tgt_grid_reward\n",
    "                self.done = True\n",
    "            elif self.state in self.forbidden_grids:\n",
    "                reward = self.forbidden_grids_penalty\n",
    "            else:\n",
    "                reward = self.step_penalty\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.full(self.grid_size, '.', dtype=object)\n",
    "        grid[self.goal_position] = 'G'\n",
    "        for f in self.forbidden_grids:\n",
    "            grid[f] = 'H'\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.state[0] * self.grid_size[1] + self.state[1]\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def vis_policy(self, q_table):\n",
    "        rows, cols = self.grid_size\n",
    "        action_maps = {0: '↑', 1: '→', 2: '↓', 3: '←', 4: '⊙'}\n",
    "        policy = np.full(self.grid_size, '⊙', dtype=object)\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                index = row * cols + col\n",
    "                if (row, col) == self.goal_position:\n",
    "                    policy[row, col] = 'G'\n",
    "                else:\n",
    "                    action = int(np.argmax(q_table[index]))\n",
    "                    policy[row, col] = action_maps.get(action, '?')\n",
    "        print(policy)\n",
    "\n",
    "\n",
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_layers: list[int], output_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        layers = []\n",
    "        in_dim = embed_dim\n",
    "        for h in hidden_layers:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU()]\n",
    "            in_dim = h\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(in_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x).squeeze(1) if x.dim() == 2 else self.embed(x)\n",
    "        h = self.net(e)\n",
    "        logits = self.out(h)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_layers: list[int]):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        layers = []\n",
    "        in_dim = embed_dim\n",
    "        for h in hidden_layers:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU()]\n",
    "            in_dim = h\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(in_dim, 1)  # Output single value V(s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x).squeeze(1) if x.dim() == 2 else self.embed(x)\n",
    "        h = self.net(e)\n",
    "        value = self.out(h).squeeze(-1)  # Return scalar value\n",
    "        return value\n",
    "\n",
    "\n",
    "class A2CSolver:\n",
    "    def __init__(self, grid_size: tuple, goal_position: tuple, forbidden_grids: list[tuple], action_space: int,\n",
    "                 hidden_layers: list[int], device: torch.device, actor_lr: float = 1e-3, critic_lr: float = 1e-3,\n",
    "                 embed_dim: int = 32, forbidden_grids_penalty: int = -2, tgt_grid_reward: int = 10, step_penalty: int = -1):\n",
    "        self.device = device\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = action_space\n",
    "        self._init_env(grid_size, goal_position, forbidden_grids, action_space,\n",
    "                      forbidden_grids_penalty, tgt_grid_reward, step_penalty)\n",
    "        self._init_model(hidden_layers, embed_dim)\n",
    "        self._init_trainer(actor_lr, critic_lr)\n",
    "\n",
    "    def _init_env(self, grid_size: tuple, goal_position: tuple, forbidden_grids: list[tuple], action_space: int = 5,\n",
    "                  forbidden_grids_penalty: int = -2, tgt_grid_reward: int = 10, step_penalty: int = -1):\n",
    "        self.env = CustomGridWorld(grid_size=grid_size, goal_position=goal_position, forbidden_grids=forbidden_grids,\n",
    "                                  action_space=action_space, forbidden_grids_penalty=forbidden_grids_penalty,\n",
    "                                  tgt_grid_reward=tgt_grid_reward, step_penalty=step_penalty)\n",
    "        self.n_observations = self.env.observation_space.n\n",
    "\n",
    "    def _init_model(self, hidden_layers: list[int], embed_dim: int):\n",
    "        self.actor = ActorModel(self.n_observations, embed_dim, hidden_layers, self.action_space).to(self.device)\n",
    "        self.critic = CriticModel(self.n_observations, embed_dim, hidden_layers).to(self.device)\n",
    "\n",
    "    def _init_trainer(self, actor_lr: float, critic_lr: float):\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    def _generate_episode(self, n_steps: int, random_start: bool = False):\n",
    "        start = 0 if not random_start else random.choice(list(range(self.n_observations)))\n",
    "        current_observation = self.env.reset(start)\n",
    "        episode_data = []\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            state = torch.tensor([current_observation], dtype=torch.long, device=self.device)\n",
    "            action_probs = self.actor(state)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample().squeeze(0)\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            next_observation, reward, done, _ = self.env.step(int(action.item()))\n",
    "            episode_data.append((current_observation, action, log_prob, reward, next_observation))\n",
    "\n",
    "            current_observation = next_observation\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return episode_data\n",
    "\n",
    "    def solve(self, n_steps: int, n_episodes: int, gamma: float, random_start: bool = False, vis_policy: bool = True, log_iters: int = 100):\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "        pbar = tqdm(range(n_episodes))\n",
    "        for n_episode in pbar:\n",
    "            episode_data = self._generate_episode(n_steps, random_start)\n",
    "            if len(episode_data) <= 1:\n",
    "                continue\n",
    "\n",
    "            # ====== Advantage Actor-Critic (A2C) ======\n",
    "            critic_losses, actor_losses = [], []\n",
    "            rewards = [r for *_, r, _ in episode_data]\n",
    "\n",
    "            # Compute discounted returns\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # Compute values and advantages\n",
    "            values = []\n",
    "            for t, (s, a, log_prob, r, s_next) in enumerate(episode_data):\n",
    "                state_tensor = torch.tensor([s], dtype=torch.long, device=self.device)\n",
    "                value = self.critic(state_tensor)\n",
    "                values.append(value)\n",
    "\n",
    "            values = torch.stack(values)\n",
    "            advantages = returns - values.detach()\n",
    "\n",
    "            # Update Critic and Actor\n",
    "            for t, (s, a, log_prob, r, s_next) in enumerate(episode_data):\n",
    "                # Critic loss: minimize MSE between predicted value and actual return\n",
    "                critic_loss = (values[t] - returns[t]).pow(2)\n",
    "                critic_losses.append(critic_loss)\n",
    "\n",
    "                # Actor loss: policy gradient using advantage\n",
    "                # Add entropy bonus for exploration\n",
    "                entropy_bonus = 0.01 * torch.log(torch.tensor(self.action_space, dtype=torch.float32))\n",
    "                actor_loss = -log_prob * advantages[t] + entropy_bonus\n",
    "                actor_losses.append(actor_loss)\n",
    "\n",
    "            # Batch update\n",
    "            if critic_losses and actor_losses:\n",
    "                critic_loss = torch.stack(critic_losses).mean()\n",
    "                actor_loss = torch.stack(actor_losses).mean()\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "            if (n_episode + 1) % log_iters == 0:\n",
    "                avg_reward = np.mean(rewards) if rewards else 0\n",
    "                pbar.set_description(\n",
    "                    f'Episode {n_episode + 1}/{n_episodes}, critic_loss: {critic_loss.item():.4f}, actor_loss: {actor_loss.item():.4f}, avg_reward: {avg_reward:.3f}'\n",
    "                )\n",
    "\n",
    "        print(\"Training Done!\")\n",
    "        if vis_policy:\n",
    "            print('Rendering final policy...')\n",
    "            self.vis_policy()\n",
    "        print('All done!')\n",
    "\n",
    "    def create_fake_qtable(self):\n",
    "        self.actor.eval()\n",
    "        fake_q_table = np.zeros([self.n_observations, self.action_space], dtype=float)\n",
    "        with torch.no_grad():\n",
    "            states = torch.arange(self.n_observations, dtype=torch.long, device=self.device)\n",
    "            action_probs = []\n",
    "            batch = 256\n",
    "            for i in range(0, len(states), batch):\n",
    "                b = states[i:i + batch].unsqueeze(1)\n",
    "                probs = self.actor(b)\n",
    "                action_probs.append(probs.cpu().numpy())\n",
    "            action_probs = np.vstack(action_probs)\n",
    "            # Use action probabilities as fake Q-values for visualization\n",
    "            fake_q_table[:] = action_probs\n",
    "        return fake_q_table\n",
    "\n",
    "    def vis_policy(self):\n",
    "        self.env.render()\n",
    "        fake_q_table = self.create_fake_qtable()\n",
    "        self.env.vis_policy(fake_q_table)\n",
    "        self.analyze_strategy()\n",
    "\n",
    "    def analyze_strategy(self):\n",
    "        \"\"\"Analyze the learned strategy in detail\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STRATEGY ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Test policy from all starting positions\n",
    "        self.actor.eval()\n",
    "        success_count = 0\n",
    "        forbidden_hits = 0\n",
    "        total_steps = 0\n",
    "        stuck_positions = 0\n",
    "\n",
    "        for start in range(self.n_observations):\n",
    "            obs = self.env.reset(start)\n",
    "            path = [obs]  # observations only\n",
    "            actions = []\n",
    "            steps = 0\n",
    "            hit_forbidden = False\n",
    "            stuck_count = 0\n",
    "\n",
    "            for step in range(200):  # Increased max steps for complex navigation\n",
    "                state_tensor = torch.tensor([obs], dtype=torch.long, device=self.device)\n",
    "                with torch.no_grad():\n",
    "                    action_probs = self.actor(state_tensor)\n",
    "                    action = action_probs.argmax(dim=1).item()\n",
    "\n",
    "                old_obs = obs\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                path.append(obs)\n",
    "                actions.append(action)\n",
    "                steps += 1\n",
    "\n",
    "                # Check if stuck in loop\n",
    "                if obs == old_obs:\n",
    "                    stuck_count += 1\n",
    "                    if stuck_count > 10:  # Consider stuck after 10 non-moves\n",
    "                        stuck_positions += 1\n",
    "                        break\n",
    "                else:\n",
    "                    stuck_count = 0\n",
    "\n",
    "                if reward == self.env.forbidden_grids_penalty:\n",
    "                    hit_forbidden = True\n",
    "                    forbidden_hits += 1\n",
    "\n",
    "                if done:\n",
    "                    if reward == self.env.tgt_grid_reward:\n",
    "                        success_count += 1\n",
    "                    break\n",
    "\n",
    "            total_steps += steps\n",
    "\n",
    "            # Print detailed analysis for first few starting positions\n",
    "            if start < 5:\n",
    "                start_pos = self.env._get_state(start)\n",
    "                final_success = reward == self.env.tgt_grid_reward if 'reward' in locals() else False\n",
    "                print(f\"Start {start} ({start_pos}): Steps={steps}, Success={final_success}, Forbidden={hit_forbidden}, Stuck={steps >= 200}\")\n",
    "                print(f\"  Path: {path[:15]}...\")\n",
    "                print(f\"  Actions: {actions[:15]}...\")\n",
    "\n",
    "                # Show if path goes through forbidden areas\n",
    "                path_coords = [self.env._get_state(obs) for obs in path]\n",
    "                forbidden_in_path = [coord for coord in path_coords if coord in self.env.forbidden_grids]\n",
    "                if forbidden_in_path:\n",
    "                    print(f\"  Forbidden areas visited: {forbidden_in_path}\")\n",
    "\n",
    "        success_rate = success_count / self.n_observations\n",
    "        forbidden_rate = forbidden_hits / self.n_observations\n",
    "        stuck_rate = stuck_positions / self.n_observations\n",
    "        avg_steps = total_steps / self.n_observations if total_steps > 0 else 200\n",
    "\n",
    "        print(f\"\\nOverall Performance:\")\n",
    "        print(f\"  Success Rate: {success_rate:.3f} ({success_count}/{self.n_observations})\")\n",
    "        print(f\"  Forbidden Hit Rate: {forbidden_rate:.3f} ({forbidden_hits}/{self.n_observations})\")\n",
    "        print(f\"  Stuck Rate: {stuck_rate:.3f} ({stuck_positions}/{self.n_observations})\")\n",
    "        print(f\"  Average Steps: {avg_steps:.1f}\")\n",
    "        print(f\"  Goal Position: {self.env.goal_position}\")\n",
    "        print(f\"  Forbidden Areas: {list(self.env.forbidden_grids)}\")\n",
    "\n",
    "        # Environment difficulty analysis\n",
    "        print(f\"\\nEnvironment Difficulty:\")\n",
    "        total_forbidden = len(self.env.forbidden_grids)\n",
    "        total_cells = self.env.grid_size[0] * self.env.grid_size[1]\n",
    "        forbidden_ratio = total_forbidden / total_cells\n",
    "        print(f\"  Grid Size: {self.env.grid_size}\")\n",
    "        print(f\"  Forbidden Ratio: {forbidden_ratio:.3f} ({total_forbidden}/{total_cells})\")\n",
    "        print(f\"  Target Reward: {self.env.tgt_grid_reward}\")\n",
    "        print(f\"  Forbidden Penalty: {self.env.forbidden_grids_penalty}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid_size = (5, 5)\n",
    "    goal_position = (4, 4)  # Move to corner away from forbidden areas\n",
    "    forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]  # Keep complex forbidden layout\n",
    "    action_space = 5\n",
    "    hidden_layers = [128, 128]  # Larger network to handle complex environment\n",
    "    embed_dim = 32\n",
    "    # Reward configuration - optimized for complex environment\n",
    "    forbidden_grids_penalty = -5  # Moderate penalty\n",
    "    tgt_grid_reward = 100  # Much higher reward to offset difficulty\n",
    "    step_penalty = -0.2  # Small penalty to encourage exploration  \n",
    "    device = torch.device('cpu')\n",
    "    actor_lr = 2e-4  # Lower for more stable learning\n",
    "    critic_lr = 1e-4  # Lower for better value estimation  \n",
    "    solver = A2CSolver(\n",
    "        grid_size, goal_position, forbidden_grids,\n",
    "        action_space, hidden_layers, device, actor_lr, critic_lr,\n",
    "        embed_dim, forbidden_grids_penalty, tgt_grid_reward, step_penalty\n",
    "    )\n",
    "    n_steps = 500\n",
    "    n_episodes = 8000  # More episodes for complex environment\n",
    "    log_iters = 100\n",
    "    gamma = 0.99  # Higher discount factor for long-term planning\n",
    "    random_start = True  # Enable random starts for better generalization\n",
    "    vis_policy = True\n",
    "    solver.solve(n_steps, n_episodes, gamma, random_start, vis_policy, log_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f842e3-eb00-4206-8aaa-877be5fed6a8",
   "metadata": {},
   "source": [
    "![](assets/result2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510f832-072b-43aa-85ab-974cfe09ed05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db83f36-7d92-42ea-a11d-331b51f6a339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
