{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915b8e6",
   "metadata": {},
   "source": [
    "### 策略迭代算法(蒙特卡洛)伪代码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df412",
   "metadata": {},
   "source": [
    "![](assets/117.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376c019",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5de0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/118.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4fff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/119.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40cb2e-2cd1-45f4-9a25-74a17fd2e194",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/120.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04edfc-9c64-47c0-9a3c-2c175059838f",
   "metadata": {},
   "source": [
    "![](assets/121.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80150de1-aa81-45c8-a8c6-c66d4d7700d3",
   "metadata": {},
   "source": [
    "![](assets/123.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b0ebd-1fff-48af-9bf2-c11395678cc6",
   "metadata": {},
   "source": [
    "![](assets/122.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ba8d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 上述例子代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC-Basic算法完整代码实现流程：\n",
    "# # initialize a policy(actions of all the states)\n",
    "# for i in n_outer_iterations:\n",
    "#   for s in every states:\n",
    "#       # fix the actions of all the states except current\n",
    "#       for a in every actions:\n",
    "#           # initialize an empty returns-list\n",
    "#           for j in n_inner_iterations:\n",
    "#               # generate an episode according to the probability and compute the discounted return based on current action and all other fixed actions\n",
    "#               # append the result to the returns-list\n",
    "#           end for\n",
    "#           # get the action value for current action (expectation of all the discounted returns),\n",
    "#           # namely, compute the mean value of the returns-list\n",
    "#       end for\n",
    "#   end for\n",
    "#   # update the policy according to the action value\n",
    "# end for\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def solver():\n",
    "    # 该代码只用MC-Basic算法更新当前例子中的s1状态处的策略\n",
    "    r_boundary = -1\n",
    "    r_target = 1\n",
    "    r_others = 0\n",
    "    gamma = 0.9\n",
    "    actions_map = {0: 'up', 1: 'right', 2: 'down', 3: 'left', 4: 'unchanged'}\n",
    "\n",
    "    # 该例子中当前策略下s1-s9处的reward\n",
    "    r_pi_k_table = [-1, 0, -1, 0, 0, 1, 0, 1, 1]\n",
    "    q_pi0_s1a1 = -1 + gamma * (-1 / (1 - gamma))\n",
    "    q_pi0_s1a2 = 0 + gamma ** 3 * (1 / (1 - gamma))\n",
    "    q_pi0_s1a3 = 0 + gamma ** 3 * (1 / (1 - gamma))\n",
    "    q_pi0_s1a4 = -1 + gamma * (-1 / (1 - gamma))\n",
    "    q_pi0_s1a5 = 0 + gamma * (-1 / (1 - gamma))\n",
    "    \n",
    "    pi_0_s1 = np.array([q_pi0_s1a1, q_pi0_s1a2, q_pi0_s1a3, q_pi0_s1a4, q_pi0_s1a5]).argmax()\n",
    "    print(f\"Best action for state s1 is: {actions_map[pi_0_s1]}\")\n",
    "    \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    solver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
