{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915b8e6",
   "metadata": {},
   "source": [
    "### Q-learning公式（off-policy版本）："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df412",
   "metadata": {},
   "source": [
    "![](assets/200.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011706a4-4eb2-466d-8640-dd0fde37493d",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1e754-c88f-4c29-87c1-6d88365d85e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](assets/202.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bf0cc-267a-407f-b59a-118ae74e68c3",
   "metadata": {},
   "source": [
    "### 上述例子代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348c7ff-9bd7-4e24-9b5b-ca683707437d",
   "metadata": {},
   "source": [
    "例子：\n",
    "使用gym仿真库，gym官网: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb6df66-515e-4a09-92bd-2e4834cbd769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gym==0.15.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.16.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (1.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\root\\anaconda3\\lib\\site-packages (from gym==0.15.4) (4.8.1.78)\n",
      "Requirement already satisfied: future in c:\\users\\root\\anaconda3\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.18.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd69e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A F F F F\n",
      "F H H F F\n",
      "F F H F F\n",
      "F H G H F\n",
      "F H F F F\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate episode:   4%|██▌                                                         | 43/1000 [03:27<1:17:01,  4.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 153\u001b[0m\n\u001b[0;32m    149\u001b[0m     env\u001b[38;5;241m.\u001b[39mvis_policy(q_table)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     solver()\n",
      "Cell \u001b[1;32mIn[1], line 128\u001b[0m, in \u001b[0;36msolver\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[0;32m    127\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m--> 128\u001b[0m     next_observation, reward, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    129\u001b[0m     ose \u001b[38;5;241m=\u001b[39m one_step_experience(current_observation\u001b[38;5;241m=\u001b[39mcurrent_observation, next_observation\u001b[38;5;241m=\u001b[39mnext_observation, action\u001b[38;5;241m=\u001b[39maction, reward\u001b[38;5;241m=\u001b[39mreward)\n\u001b[0;32m    130\u001b[0m     current_observation \u001b[38;5;241m=\u001b[39m next_observation\n",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m, in \u001b[0;36mCustomGridWorld.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation()\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes one step in the environment\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='CustomGridWorld-v0',\n",
    "    entry_point='__main__:CustomGridWorld',\n",
    ")\n",
    "\n",
    "class CustomGridWorld(gym.Env):\n",
    "    def __init__(self, grid_size=(5, 5), goal_position=(3, 2), forbidden_grids=None, action_space=5):\n",
    "        super(CustomGridWorld, self).__init__()\n",
    "        # Grid size (rows, columns)\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_position = goal_position\n",
    "        # Define action space: up, right, down, left, unchanged (5 actions)\n",
    "        self.action_space = spaces.Discrete(action_space)\n",
    "        # Observation space: grid positions, represented as a flat space\n",
    "        self.observation_space = spaces.Discrete(grid_size[0] * grid_size[1])\n",
    "        # Initialize agent's starting position (top-left corner)\n",
    "        self.state = (0, 0)\n",
    "        self.done = False\n",
    "        # Set the forbidden grids (if not specified, use a default list)\n",
    "        if forbidden_grids is None:\n",
    "            forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]  # Given forbidden grids\n",
    "        self.forbidden_grids = set(forbidden_grids)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state\"\"\"\n",
    "        self.state = (0, 0)  # Reset the agent to the top-left corner\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes one step in the environment\"\"\"\n",
    "        if self.done:\n",
    "            return self._get_observation(), 1, True, {}\n",
    "\n",
    "        x, y = self.state\n",
    "        # Define movement based on action\n",
    "        if action == 0:  # Up\n",
    "            new_x = max(0, x - 1)\n",
    "            new_y = y\n",
    "        elif action == 1:  # Right\n",
    "            new_x = x\n",
    "            new_y = min(self.grid_size[1] - 1, y + 1)\n",
    "        elif action == 2:  # Down\n",
    "            new_x = min(self.grid_size[0] - 1, x + 1)\n",
    "            new_y = y\n",
    "        elif action == 3:  # Left\n",
    "            new_x = x\n",
    "            new_y = max(0, y - 1)\n",
    "        elif action == 4:  # Unchanged (stay in the same position)\n",
    "            new_x = x\n",
    "            new_y = y\n",
    "\n",
    "        # Check if the new position is out of bounds\n",
    "        if new_x < 0 or new_x >= self.grid_size[0] or new_y < 0 or new_y >= self.grid_size[1]:\n",
    "            reward = -1  # Penalty for trying to go out of bounds\n",
    "            self.state = (x, y)  # Keep the agent at the same position\n",
    "        else:\n",
    "            self.state = (new_x, new_y)\n",
    "            # Check if the agent reached the goal\n",
    "            if self.state == self.goal_position:\n",
    "                reward = 1\n",
    "                self.done = True\n",
    "            # Check if the agent stepped into a forbidden grid\n",
    "            elif self.state in self.forbidden_grids:\n",
    "                reward = -1  # Penalty for entering a forbidden grid\n",
    "            else:\n",
    "                reward = 0  # No penalty for regular move\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment (prints the grid)\"\"\"\n",
    "        grid = np.full(self.grid_size, 'F', dtype=object)  # Default is frozen\n",
    "        # Set goal, forbidden grids, and agent position\n",
    "        grid[self.goal_position] = 'G'\n",
    "        for f in self.forbidden_grids:\n",
    "            grid[f] = 'H'  # H for hole (forbidden grid)\n",
    "        # Print the grid with agent position\n",
    "        grid[self.state] = 'A'\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Returns the current state as a flat index\"\"\"\n",
    "        return self.state[0] * self.grid_size[1] + self.state[1]\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the environment\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def vis_policy(self, q_table):\n",
    "        self.render()\n",
    "        action_maps = {0: '↑', 1: '→', 2: '↓', 3: '←', 4: '⊙'}\n",
    "        policy = np.full(self.grid_size, '⊙', dtype=object)\n",
    "        for row in range(self.grid_size[0]):\n",
    "            for col in range(self.grid_size[1]):\n",
    "                index = row * self.grid_size[0] + col\n",
    "                action = q_table[index].argmax()\n",
    "                policy[row, col] = action_maps[action]\n",
    "        print(policy)\n",
    "\n",
    "one_step_experience = namedtuple('one_step_experience', field_names=['current_observation', 'next_observation', 'action', 'reward'])\n",
    "\n",
    "def solver():\n",
    "    grid_size = (5, 5)\n",
    "    goal_position = (3, 2)\n",
    "    forbidden_grids = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]\n",
    "    action_space = 5\n",
    "    env = CustomGridWorld(grid_size=grid_size, goal_position=goal_position, forbidden_grids=forbidden_grids, action_space=action_space)\n",
    "    env.render()\n",
    "    \n",
    "    n_episodes = 1000\n",
    "    n_steps = 1000000\n",
    "    policies_b = []\n",
    "    for _ in tqdm(range(n_episodes), desc='generate episode'):\n",
    "        policy_b = []\n",
    "        current_observation = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_observation, reward, _, _ = env.step(action)\n",
    "            ose = one_step_experience(current_observation=current_observation, next_observation=next_observation, action=action, reward=reward)\n",
    "            current_observation = next_observation\n",
    "            policy_b.append(ose)\n",
    "        policies_b.append(policy_b)\n",
    "    \n",
    "    q_table = np.zeros([grid_size[0] * grid_size[1], action_space], dtype=np.float32)\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    for i_episode in tqdm(range(n_episodes), desc='update policy'):\n",
    "        policy_b = policies_b[i_episode]\n",
    "        for i_step in range(n_steps):\n",
    "            ose = policy_b[i_step]\n",
    "            current_observation = ose.current_observation\n",
    "            next_observation = ose.next_observation\n",
    "            action = ose.action\n",
    "            reward = ose.reward\n",
    "            q_table[current_observation, action] = q_table[current_observation, action] - alpha * (q_table[current_observation, action] - (reward + gamma * q_table[next_observation, :].max()))\n",
    "        \n",
    "    print(q_table)\n",
    "    print(q_table.argmax(1))\n",
    "    env.vis_policy(q_table)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e23e32-1dd1-4408-9d5d-090b2c145b04",
   "metadata": {},
   "source": [
    "由于步数太多，直接展示运行结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e86f4a-e438-4c32-9aa8-8baf929d3dfc",
   "metadata": {},
   "source": [
    "![](assets/result.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e441070-5af5-463b-8038-116a925e4fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
